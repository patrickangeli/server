name: Monitor Websites

on:
  schedule:
    - cron: '*/30 * * * *'  # Executa a cada 30 minutos
  workflow_dispatch:  # Permite execu√ß√£o manual

jobs:
  check-websites:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Set up Ruby
        uses: ruby/setup-ruby@v1
        with:
          ruby-version: '3.0'

      - name: Create Jekyll structure
        run: |
          mkdir -p docs
          cd docs
          mkdir -p _sass assets/css
          echo 'source: .' > _config.yml
          echo 'sass:' >> _config.yml
          echo '  style: compressed' >> _config.yml
          echo '  sass_dir: _sass' >> _config.yml
          echo '@import "main";' > assets/css/style.scss
          echo '// Base styles' > _sass/main.scss

      - name: Install Jekyll and dependencies
        run: |
          cd docs
          echo 'source "https://rubygems.org"' > Gemfile
          echo 'gem "github-pages"' >> Gemfile
          echo 'gem "jekyll"' >> Gemfile
          echo 'gem "minima"' >> Gemfile
          echo 'gem "faraday-retry"' >> Gemfile
          bundle install

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.x'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4

      - name: Check websites for changes
        env:
          TELEGRAM_TOKEN: ${{ secrets.TELEGRAM_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
          WEBSITE_URLS: ${{ secrets.WEBSITE_URLS }}
        run: |
          import requests
          from bs4 import BeautifulSoup
          import os
          import hashlib
          import json
          from datetime import datetime

          # Cria a pasta 'docs' se n√£o existir
          pasta_docs = 'docs'
          if not os.path.exists(pasta_docs):
              os.makedirs(pasta_docs)
              print(f'A pasta "{pasta_docs}" foi criada.')
          else:
              print(f'A pasta "{pasta_docs}" j√° existe.')

          # Configura o arquivo de log
          log_file = 'docs/change_log.json'
          if not os.path.exists(log_file):
              with open(log_file, 'w') as f:
                  json.dump([], f)

          def send_telegram_message(message):
              token = os.environ['TELEGRAM_TOKEN']
              chat_id = os.environ['TELEGRAM_CHAT_ID']
              url = f"https://api.telegram.org/bot{token}/sendMessage"
              payload = {
                  "chat_id": chat_id,
                  "text": message,
                  "parse_mode": "HTML"
              }
              try:
                  response = requests.post(url, json=payload)
                  response.raise_for_status()
              except Exception as e:
                  print(f"Erro ao enviar mensagem: {str(e)}")

          def check_website(url):
              try:
                  headers = {
                      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                  }
                  response = requests.get(url, headers=headers, timeout=30)
                  response.raise_for_status()
                  soup = BeautifulSoup(response.text, 'html.parser')
                  content = soup.get_text()
                  return hashlib.md5(content.encode()).hexdigest()
              except Exception as e:
                  print(f"Erro ao verificar {url}: {str(e)}")
                  return None

          # Carregar URLs dos sites
          websites = json.loads(os.environ['WEBSITE_URLS'])

          # Carregar hashes anteriores
          try:
              with open('docs/last_hashes.json', 'r') as f:
                  last_hashes = json.load(f)
          except FileNotFoundError:
              last_hashes = {}

          current_hashes = {}
          updates = []
          timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

          for url in websites:
              current_hash = check_website(url)
              if current_hash:
                  current_hashes[url] = current_hash
                  if url not in last_hashes or current_hash != last_hashes[url]:
                      update_message = f"üîÑ <b>Atualiza√ß√£o Detectada!</b>\n\nüì± Site: {url}\n‚è∞ Data: {timestamp}"
                      updates.append(update_message)

          if updates:
              send_telegram_message("\n\n".join(updates))

              # Salvar as mudan√ßas no log
              with open(log_file, 'r+') as f:
                  logs = json.load(f)
                  for update in updates:
                      logs.append({
                          'timestamp': timestamp,
                          'update': update
                      })
                  f.seek(0)
                  json.dump(logs, f, indent=2)

          # Salvar os novos hashes
          with open('docs/last_hashes.json', 'w') as f:
              json.dump(current_hashes, f, indent=2)

          if not updates:
              print(f"[{timestamp}] Nenhuma mudan√ßa detectada.")
        shell: python

      - name: Build Jekyll site
        run: |
          cd docs
          bundle exec jekyll build

      - name: Deploy to GitHub Pages
        if: success()
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./docs/_site
